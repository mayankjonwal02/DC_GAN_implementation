{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CycleGAN From Scratch with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpEB1WLrhOx1"
      },
      "source": [
        "### Dataset - Content\n",
        "\n",
        "### [Link to Dataset](https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/)\n",
        "\n",
        "Summer2Winter Yosemite dataset consists of 1540 Summer Photos & 1200 Winter Photos with each split into train and test subsets.\n",
        "\n",
        "\n",
        "\n",
        "This dataset was obtained from UC Berkeley's official directory of CycleGAN Datasets. For more details on the dataset refer the related CycleGAN publication. Work based on the dataset should cite:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T13:53:31.516722Z",
          "iopub.status.busy": "2022-03-16T13:53:31.516174Z",
          "iopub.status.idle": "2022-03-16T13:53:33.116422Z",
          "shell.execute_reply": "2022-03-16T13:53:33.115657Z",
          "shell.execute_reply.started": "2022-03-16T13:53:31.516622Z"
        },
        "id": "jcOwebqPwKux",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import itertools\n",
        "import scipy\n",
        "import sys\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "import torch.autograd as autograd\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import glob\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T13:53:33.989252Z",
          "iopub.status.busy": "2022-03-16T13:53:33.988847Z",
          "iopub.status.idle": "2022-03-16T13:53:33.999219Z",
          "shell.execute_reply": "2022-03-16T13:53:33.998273Z",
          "shell.execute_reply.started": "2022-03-16T13:53:33.989215Z"
        },
        "id": "rVBTY_4wwfbq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# HYPERPARAMETERS \n",
        "class Hyperparameters(object):\n",
        "  def __init__(self, **kwargs):\n",
        "    self.__dict__.update(kwargs)\n",
        "\n",
        "hp = Hyperparameters(\n",
        "    epoch=0,\n",
        "    n_epochs=200,    \n",
        "    dataset_train_mode=\"train\",\n",
        "    dataset_test_mode=\"test\", \n",
        "    batch_size=4,        \n",
        "    lr=.0002,\n",
        "    decay_start_epoch=100,\n",
        "    b1=.5,\n",
        "    b2=0.999,\n",
        "    n_cpu=8,\n",
        "    img_size=128,\n",
        "    channels=3,\n",
        "    n_critic=5,\n",
        "    sample_interval=100,\n",
        "    num_residual_blocks=19,\n",
        "    lambda_cyc=10.0,\n",
        "    lambda_id=5.0)\n",
        "\n",
        "# Root Path for Google Drive\n",
        "root_path = 'cycle_data'\n",
        "\n",
        "# Kaggle\n",
        "# root_path = '../input/summer2winter-yosemite'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T13:53:34.004777Z",
          "iopub.status.busy": "2022-03-16T13:53:34.004129Z",
          "iopub.status.idle": "2022-03-16T13:53:34.018965Z",
          "shell.execute_reply": "2022-03-16T13:53:34.018189Z",
          "shell.execute_reply.started": "2022-03-16T13:53:34.004710Z"
        },
        "id": "yZaSsdJa_xqH",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def convert_to_rgb(image):\n",
        "    rgb_image = Image.new(\"RGB\", image.size)\n",
        "    rgb_image.paste(image)\n",
        "    return rgb_image\n",
        "\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root, transforms_=None, unaligned=False, mode=\"train\"):\n",
        "        self.transform = transforms.Compose(transforms_)\n",
        "        self.unaligned = unaligned        \n",
        "\n",
        "        self.files_A = sorted(glob.glob(os.path.join(root, \"%sA\" % mode) + \"/*.*\"))\n",
        "        self.files_B = sorted(glob.glob(os.path.join(root, \"%sB\" % mode) + \"/*.*\"))\n",
        "        # print(\"self.files_B \", self.files_B)\n",
        "        ''' Will print below array with all file names\n",
        "        ['/content/drive/MyDrive/All_Datasets/summer2winter_yosemite/trainB/2005-06-26 14:04:52.jpg', \n",
        "        '/content/drive/MyDrive/All_Datasets/summer2winter_yosemite/trainB/2005-08-02 09:19:52.jpg',..]\n",
        "        '''\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_A = Image.open(self.files_A[index % len(self.files_A)])\n",
        "        # a % b => a is divided by b, and the remainder of that division is returned.\n",
        "\n",
        "        if self.unaligned:\n",
        "            image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\n",
        "        else:\n",
        "            image_B = Image.open(self.files_B[index % len(self.files_B)])\n",
        "\n",
        "        # Convert grayscale images to rgb\n",
        "        if image_A.mode != \"RGB\":\n",
        "            image_A = convert_to_rgb(image_A)\n",
        "        if image_B.mode != \"RGB\":\n",
        "            image_B = convert_to_rgb(image_B)\n",
        "\n",
        "        item_A = self.transform(image_A)\n",
        "        item_B = self.transform(image_B)\n",
        "        \n",
        "        # Finally ruturn a dict\n",
        "        return {\"A\": item_A, \"B\": item_B}\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(len(self.files_A), len(self.files_B))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08V7JDt_J4KQ"
      },
      "source": [
        "### Bonus Point - How does a % b works when a is smaller than b\n",
        "\n",
        "https://stackoverflow.com/questions/1535656/how-does-a-modulo-operation-work-when-the-first-number-is-smaller\n",
        "\n",
        "for instance\n",
        "\n",
        "2 % 5 the answer is 2.\n",
        "\n",
        "2 divided by 5 (integer division) is 0 with a remainder of 2.\n",
        "\n",
        "2 = 0 x 5 + 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE3FQfJSJ4KQ",
        "outputId": "6cd04147-663f-4376-fa5f-2d77a73b0370"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "2\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "print(2 % 5)\n",
        "print(2 % 8)\n",
        "print(2 % 15000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2022-03-16T13:53:34.020893Z",
          "iopub.status.busy": "2022-03-16T13:53:34.020275Z",
          "iopub.status.idle": "2022-03-16T13:53:34.033695Z",
          "shell.execute_reply": "2022-03-16T13:53:34.033041Z",
          "shell.execute_reply.started": "2022-03-16T13:53:34.020855Z"
        },
        "id": "VWcpYFFnWAbv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Methods for Image Visualization\n",
        "\n",
        "def show_img(img,size=10):\n",
        "  img = img / 2 + 0.5     \n",
        "  npimg = img.numpy()\n",
        "  plt.figure(figsize=(size, size))\n",
        "  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "  plt.show()\n",
        "'''  The reason for doing \"np.transpose(npimg, (1, 2, 0))\"\n",
        "\n",
        "PyTorch modules processing image data expect tensors in the format C × H × W.\n",
        "Whereas PILLow and Matplotlib expect image arrays in the format H × W × C\n",
        "so to use them with matplotlib you need to reshape it\n",
        "to put the channels as the last dimension:\n",
        "\n",
        "I could have used permute() method as well like below \n",
        "plt.imshow(pytorch_tensor_image.permute(1, 2, 0))\n",
        "'''\n",
        "\n",
        "def to_img(x):    \n",
        "    x = x.view(x.size(0)*2, hp.channels, hp.img_size, hp.img_size)\n",
        "    return x\n",
        "\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "def plot_output(path, x, y):\n",
        "    img = mpimg.imread(path)\n",
        "    plt.figure(figsize=(x,y))\n",
        "    plt.imshow(img)  \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVoxxG01hOx7"
      },
      "source": [
        "## Get Train and Validation Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-03-16T13:53:34.035537Z",
          "iopub.status.busy": "2022-03-16T13:53:34.035062Z",
          "iopub.status.idle": "2022-03-16T13:53:34.071627Z",
          "shell.execute_reply": "2022-03-16T13:53:34.070991Z",
          "shell.execute_reply.started": "2022-03-16T13:53:34.035503Z"
        },
        "id": "pR5LjEFQARFm",
        "outputId": "edb6d4ed-744a-4841-91a1-e7fe8a2c2c53",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "transforms_ = [\n",
        "    transforms.Resize((hp.img_size, hp.img_size), Image.BICUBIC),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "]\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    ImageDataset(root_path, mode=hp.dataset_train_mode, transforms_=transforms_),\n",
        "    batch_size=hp.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    ImageDataset(root_path, mode=hp.dataset_test_mode, transforms_=transforms_),\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uttx-WLhOx8"
      },
      "source": [
        "## Visualize some sample images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import torchvision.utils as vutils\n",
        "# device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and 1 > 0) else \"cpu\")\n",
        "\n",
        "# # Plot some training images\n",
        "# real_batch = next(iter(train_dataloader))\n",
        "# plt.figure(figsize=(8,8))\n",
        "# plt.axis(\"off\")\n",
        "# plt.title(\"Training Images\")\n",
        "# plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "execution": {
          "iopub.execute_input": "2022-03-16T13:53:34.075907Z",
          "iopub.status.busy": "2022-03-16T13:53:34.073619Z",
          "iopub.status.idle": "2022-03-16T13:53:35.132905Z",
          "shell.execute_reply": "2022-03-16T13:53:35.131542Z",
          "shell.execute_reply.started": "2022-03-16T13:53:34.075868Z"
        },
        "id": "HbxCwP0zXyBQ",
        "outputId": "24f8e681-e738-400c-f236-38d4cfcccf01",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# pic_size = 2 \n",
        "\n",
        "# dataiter = iter(train_dataloader)\n",
        "# images = dataiter.next()\n",
        "\n",
        "# for i in range(len(images[\"A\"])):\n",
        "#   show_img(make_grid([images[\"A\"][i],images[\"B\"][i]]), size=pic_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrk-ZgCtpu4K"
      },
      "source": [
        "## Replay Buffer \n",
        "\n",
        "As per the paper -  To reduce model oscillation, we update the discriminator using a history of generated images rather than the ones produced by the latest generators. We keep an image buffer that stores the 50 previously created images.\n",
        "\n",
        "And here is the link the Paper Published in 2017 by Shrivastava - https://arxiv.org/pdf/1612.07828.pdf\n",
        "\n",
        "\n",
        "This is another strategy used to stabilize the CycleGAN Training\n",
        "\n",
        "Replay buffer is used to train the discriminator. Generated images are added to the replay buffer and sampled from it.\n",
        "\n",
        "The replay buffer returns the newly added image with a probability of 0.5. \n",
        "\n",
        "Otherwise, it sends an older generated image and replaces the older image with the newly generated image.\n",
        "\n",
        "This is done to reduce model oscillation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "execution": {
          "iopub.execute_input": "2022-03-16T13:53:35.142822Z",
          "iopub.status.busy": "2022-03-16T13:53:35.140057Z",
          "iopub.status.idle": "2022-03-16T13:53:35.156894Z",
          "shell.execute_reply": "2022-03-16T13:53:35.155795Z",
          "shell.execute_reply.started": "2022-03-16T13:53:35.142749Z"
        },
        "id": "OXqLUivNDrNa",
        "outputId": "d1442e34-06a0-4c4b-da6e-8c9202bed67d",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' torch.cat: Concatenates the given sequence of seq tensors in the given dimension. The consequence is that a specific dimension changes size e.g. dim=0 then you are adding elements to the row which increases the dimensionality of the column space. '"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ReplayBuffer\n",
        "class ReplayBuffer:\n",
        "    # We keep an image buffer that stores\n",
        "    # the 50 previously created images.\n",
        "    def __init__(self, max_size=50):\n",
        "        assert max_size > 0, \"Empty buffer.\"\n",
        "        self.max_size = max_size\n",
        "        self.data = []\n",
        "\n",
        "    def push_and_pop(self, data):\n",
        "        to_return = []\n",
        "        for element in data.data:\n",
        "            element = torch.unsqueeze(element, 0)\n",
        "            if len(self.data) < self.max_size:\n",
        "                self.data.append(element)\n",
        "                to_return.append(element)\n",
        "            else:\n",
        "                # Returns newly added image with a probability of 0.5.\n",
        "                if random.uniform(0, 1) > 0.5:\n",
        "                    i = random.randint(0, self.max_size - 1)\n",
        "                    to_return.append(self.data[i].clone())\n",
        "                    self.data[\n",
        "                        i\n",
        "                    ] = element  # replaces the older image with the newly generated image.\n",
        "                else:\n",
        "                    # Otherwise, it sends an older generated image and\n",
        "                    to_return.append(element)\n",
        "        return Variable(torch.cat(to_return))\n",
        "\n",
        "\n",
        "''' torch.cat: Concatenates the given sequence of seq tensors in the given dimension. The consequence is that a specific dimension changes size e.g. dim=0 then you are adding elements to the row which increases the dimensionality of the column space. '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2a20DcIFhoY"
      },
      "source": [
        "## Learning Rate scheduling with `lr_lambda`\n",
        "\n",
        "As per the paper -  \"We keep the same learning rate\n",
        "for the first 100 epochs and linearly decay the rate to zero\n",
        "over the next 100 epochs.\"\n",
        "\n",
        "### First, I am creating a class `LambdaLR(n_epochs, offset, decay_start_epoch)` - Lets understand how its working\n",
        "\n",
        "Following the Paper, in my `LambdaLR` class the `decay_start_epoch` hyperparameter is kept at 100\n",
        "\n",
        "And then just before training, I will invoke the `LambdaLR()` method as below, to set the `lr_scheduler_G`\n",
        "\n",
        "```py\n",
        "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
        "    optimizer_G, lr_lambda=LambdaLR(hp.n_epochs, hp.epoch, hp.decay_start_epoch).step\n",
        ")\n",
        "```\n",
        "\n",
        "Where `lr_lambda` (is a function or list) – A function which computes a multiplicative factor given an integer parameter epoch, or a list of such functions, one for each group in optimizer.param_groups.\n",
        "\n",
        "So basically, below ia a simplified application of the lambda function.\n",
        "\n",
        "```py\n",
        "lambda_func = lambda epoch: 1 - max(0, epoch - decay_start_epoch)/(n_epochs - decay_start_epoch)\n",
        "\n",
        "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lambda_func)\n",
        "```\n",
        "\n",
        "### Understanding `lr_lambda` arg in `torch.optim.lr_scheduler.LambdaLR`\n",
        "\n",
        "lr_lambda Sets the learning rate of each parameter group to the initial lr times a given function. When last_epoch=-1, sets initial lr as lr.\n",
        "\n",
        "The new learning rate is always calculated like that:\n",
        "\n",
        "### lr_epoch = lr_initial ∗ Lambda(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NhzmQW_JFih3"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LambdaLR:\n",
        "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
        "        assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n",
        "        self.n_epochs = n_epochs\n",
        "        self.offset = offset\n",
        "        self.decay_start_epoch = decay_start_epoch\n",
        "\n",
        "    def step(self, epoch):\n",
        "        # Below line checks whether the current epoch has exceeded the decay epoch(which is 100)\n",
        "        # e.g. if current epoch is 80 then max (0, 80 - 100) will be 0. \n",
        "        # i.e. then entire numerator will be 0 - so 1 - 0 is 1\n",
        "        # i.e. the original LR remains as it is.\n",
        "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnNBbWlZJ4KZ"
      },
      "source": [
        "#### So e.g. for epoch=110, the above function `LambdaLR` will return as below\n",
        "\n",
        "`return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)`\n",
        "\n",
        "will be\n",
        "\n",
        "1.0 - max(0, 110 + 0 - 100) / (200 - 100) \n",
        "\n",
        "= 1.0 - max(0, 10) / (100) \n",
        "\n",
        "= 1 - 1/10 = 0.9\n",
        "\n",
        "So that means the Decay Factor of the Learning rate for epoch=110 is 0.9 i.e. the Learning rate would be reduced as below\n",
        "\n",
        "New LR = Initial LR * 0.9\n",
        "\n",
        "### Similarly for epoch=120, `LambdaLR` will return as below\n",
        "\n",
        "1.0 - max(0, 120 + 0 - 100) / (200 - 100) \n",
        "\n",
        "= 1.0 - max(0, 20) / (100) \n",
        "\n",
        "= 1 - 1/5 = 0.8\n",
        "\n",
        "So that means the Decay Factor of the Learning rate for epoch=110 is 0.9 i.e. the Learning rate would be reduced as below\n",
        "\n",
        "New LR = Initial LR * 0.8\n",
        "\n",
        "--------------------------------------\n",
        "\n",
        "#### Overall, in my `LambdaLR` class - the implementation logic goes like this\n",
        "\n",
        "- In-order to linearly decay the learning rate after 100 epoch the lambda function checks whether the current epoch has exceeds the `decay_start_epoch`(which is 100).\n",
        "\n",
        "- If current epoch is less than `decay_start_epoch`(which is 100) it returns 1. So that initial lr remain same for the first 100 epochs.\n",
        "\n",
        "- If the current epoch has exceeded the `decay_start_epoch`(which is 100) the initial lr will be decreased through out the rest of the epochs among total epochs(rest 100 epoch out of total 200 epochs of training).\n",
        "\n",
        "- If we equally divide the lr for the last 100 epochs and keep subtracting from \"Base-LR\" or \"Initial LR\", it will reach to 0 by the end of the last 100 epochs.\n",
        "\n",
        "- As lambda lr multiply initial lr with given function, epoch beyond the `decay_start_epoch` will sum up the consistent decrease in lr value from staring of decay epoch(which is 100) to the current epoch(for example 110)).\n",
        "\n",
        "As it does not have the decayed lr at previous epochs(here epoch 109 in case of current epoch 110) and only have Base lr or the Initial LR, it sum up the decrement occurred in lr for the previous epoch.\n",
        "\n",
        "--------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_ThxhkitKp3"
      },
      "source": [
        "### Initialize convolution layer weights to N(0,0.02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SAzjwwMYuTXw"
      },
      "outputs": [],
      "source": [
        "def initialize_conv_weights_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        if hasattr(m, \"bias\") and m.bias is not None:\n",
        "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE34s6vbuU_B"
      },
      "source": [
        "# GENERATOR & DISCRIMINATOR\n",
        "\n",
        "### Residual Block\n",
        "\n",
        "As per the paper - \" Reflection padding was used to reduce artifacts.\n",
        "A residual block contains two 3 × 3 convolutional layers with the same number of filters on both layer. \"\n",
        "\n",
        "\n",
        "<blockquote>\n",
        "<p>**7.2. Network architectures** -->Generator architectures -->\"We use 6 residual blocks for 128 × 128 training images, and 9 residual blocks for 256 × 256 or higher-resolution training images.\"\n",
        "\n",
        "\"Let c7s1-k denote a 7×7 Convolution-InstanceNormReLU layer with k filters and stride 1. dk denotes a 3 × 3 Convolution-InstanceNorm-ReLU layer with k filters and stride 2. Reflection padding was used to reduce artifacts. Rk denotes a residual block that contains two 3 × 3 convolutional layers with the same number of filters on both layer. uk denotes a 3 × 3 fractional-strided-ConvolutionInstanceNorm-ReLU layer with k filters.\"\n",
        "\n",
        "\"The network with 9 residual blocks consists of:\n",
        "**\"c7s1-64,d128,d256,R256,R256,R256, R256,R256,R256,R256,R256,R256,u128 u64,c7s1-3\"**</p>\n",
        "</blockquote>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4UrP6ISWJ4Kc"
      },
      "outputs": [],
      "source": [
        "##############################################\n",
        "# Residual block with two convolution layers.\n",
        "##############################################\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channel):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1), # Reflection padding is used because it gives better image quality at edges.\n",
        "            nn.Conv2d(in_channel, in_channel, 3), # Paper says - same number of filters on both layer.\n",
        "            nn.InstanceNorm2d(in_channel),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_channel, in_channel, 3),\n",
        "            nn.InstanceNorm2d(in_channel),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47ZJlXR_J4Kc"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "### Parameters in torch.nn.conv2d()\n",
        "\n",
        "```py\n",
        "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
        "\n",
        "```\n",
        "\n",
        "Where\n",
        "\n",
        "* in_channels (int) – Number of channels/filters in the input image\n",
        "\n",
        "* out_channels (int) – Number of channels/filters produced by the convolution\n",
        "\n",
        "* kernel_size (int or tuple) – Size of the convolving kernel\n",
        "\n",
        "* stride (int or tuple, optional) – Stride of the convolution. (Default: 1)\n",
        "\n",
        "* padding (int or tuple, optional) – Zero-padding added to both sides of the input (Default: 0)\n",
        "\n",
        "* padding_mode (string, optional) – zeros\n",
        "\n",
        "* dilation (int or tuple, optional) – Spacing between kernel elements. (Default: 1)\n",
        "\n",
        "* groups (int, optional) – Number of blocked connections from input to output channels. (Default: 1)\n",
        "\n",
        "* bias (bool, optional) – If True, adds a learnable bias to the output. (Default: True)\n",
        "\n",
        "---\n",
        "\n",
        "## The generator\n",
        "\n",
        "* The generator consists encoder and decoder. It downsample or encode the input image, then interpret the encoding with 9 Residual Blocks having skip connections.After that with a a series of layers it upsample or decode the representation to the size of the fake image.\n",
        "\n",
        "* Reflection padding “reflects” the row into the padding. It is used mostly for brightness, contrast and for reducing artifact.\n",
        "\n",
        "* Batch norm normalizes across the mini batch of definite size.On the other hand, Instance normalization normalizes across each channel in each data instead of normalizing across input features in a data. Instance Norm normalizes each batch independently and across spatial locations only.\n",
        "\n",
        "* Use of instance normalization layers, the normalization process allows to remove instance-specific contrast information from the image content, which simplifies image generation. Thus results in vastly improved images.\n",
        "\n",
        "![Imgur](https://imgur.com/38kq2bw.png)\n",
        "\n",
        "As you can see above, the representation size shrinks in the encoder phase, stays constant in the transformer phase, and expands again in the decoder phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T13:53:35.158755Z",
          "iopub.status.busy": "2022-03-16T13:53:35.158453Z",
          "iopub.status.idle": "2022-03-16T13:53:35.183204Z",
          "shell.execute_reply": "2022-03-16T13:53:35.181073Z",
          "shell.execute_reply.started": "2022-03-16T13:53:35.158719Z"
        },
        "id": "EEb5TdBmIy7l",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "##############################################\n",
        "# Generator\n",
        "##############################################\n",
        "\n",
        "\"\"\" As per Paper -- Generator with 9 residual blocks consists of:\n",
        "c7s1-64,d128,d256,R256,R256,R256, R256,R256,R256,R256,R256,R256,\n",
        "u128, u64,c7s1-3\n",
        " \"\"\"\n",
        "\n",
        "class GeneratorResNet(nn.Module):\n",
        "    def __init__(self, input_shape, num_residual_blocks):\n",
        "        super(GeneratorResNet, self).__init__()\n",
        "\n",
        "        channels = input_shape[0]\n",
        "\n",
        "        # Initial convolution block\n",
        "        out_channels = 64\n",
        "        # I define a variable 'model' which I will continue to update\n",
        "        # throughout the 3 blocks of Residual -> Downsampling -> Upsampling\n",
        "        # First c7s1-64\n",
        "        model = [\n",
        "            nn.ReflectionPad2d(channels),\n",
        "            nn.Conv2d(channels, out_channels, 7),\n",
        "            nn.InstanceNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "        in_channels = out_channels\n",
        "\n",
        "        # Downsampling\n",
        "        # d128 => d256\n",
        "        for _ in range(2):\n",
        "            out_channels *= 2\n",
        "            model += [\n",
        "                nn.Conv2d(in_channels, out_channels, 3, stride=2, padding=1),\n",
        "                nn.InstanceNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "            in_channels = out_channels\n",
        "\n",
        "        \"\"\" Residual blocks - Per Paper\n",
        "        R256,R256,R256,R256,R256,R256,R256,R256,R256\n",
        "        \"\"\"\n",
        "        for _ in range(num_residual_blocks):\n",
        "            model += [ResidualBlock(out_channels)]\n",
        "\n",
        "        # Upsampling\n",
        "        # u128 => u64\n",
        "        for _ in range(2):\n",
        "            out_channels //= 2\n",
        "            model += [\n",
        "                nn.Upsample(scale_factor=2),\n",
        "                nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1),\n",
        "                nn.InstanceNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "            in_channels = out_channels\n",
        "\n",
        "        # Output layer\n",
        "        model += [\n",
        "            nn.ReflectionPad2d(channels),\n",
        "            nn.Conv2d(out_channels, channels, 7),\n",
        "            nn.Tanh(),\n",
        "        ]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr6n99pAJ4Ke"
      },
      "source": [
        "#### Meaning for *model in `nn.Sequential(*model)` \n",
        "\n",
        "The syntax is to use the symbol * to take in a variable number of arguments\n",
        "\n",
        "*args allows you to do is take in more arguments than the number of formal arguments that you previously defined"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1wsIZPVJ4Ke"
      },
      "source": [
        "## Discriminator\n",
        "\n",
        "As per the Paper :\n",
        "\n",
        "<blockquote>\n",
        "<p>\n",
        "\n",
        "**7.2. Network architectures - Discriminator architectures**\n",
        "\n",
        "\"For discriminator networks, we use 70 × 70 PatchGAN [22]. Let Ck denote a 4 × 4 Convolution-InstanceNorm-LeakyReLU layer with k filters and stride 2. After the last layer, we apply a convolution to produce a 1-dimensional output. We do not use InstanceNorm for the first C64 layer. We use leaky ReLUs with a slope of 0.2. The discriminator architecture is:\" C64-C128-C256-C512\n",
        "<p/>\n",
        "<blockquote/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGBzjPnNJ4Ke"
      },
      "source": [
        "Not listed in the paper, the model also has a final hidden layer C512 with a 1×1 stride. \n",
        "\n",
        "Given the model is mostly used with 256×256 sized images as input, the size of the output feature map of activations is 16×16. If 128×128 images were used as input, then the size of the output feature map of activations would be 8×8.\n",
        "\n",
        "![Imgur](https://imgur.com/ti5xUof.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GttrM1PTJ4Kf"
      },
      "outputs": [],
      "source": [
        "\n",
        "##############################\n",
        "#        Discriminator\n",
        "##############################\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_shape):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        channels, height, width = input_shape\n",
        "\n",
        "        # Calculate output shape of image discriminator (PatchGAN)\n",
        "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
        "\n",
        "        def discriminator_block(in_channels, out_channels, normalize=True):\n",
        "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
        "            layers = [nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)]\n",
        "            if normalize:\n",
        "                layers.append(nn.InstanceNorm2d(out_channels))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        # C64 -> C128 -> C256 -> C512\n",
        "        self.model = nn.Sequential(\n",
        "            *discriminator_block(channels, out_channels=64, normalize=False),\n",
        "            *discriminator_block(64, out_channels=128),\n",
        "            *discriminator_block(128, out_channels=256),\n",
        "            *discriminator_block(256, out_channels=512),\n",
        "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
        "            nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, padding=1)\n",
        "        )\n",
        "        # With nn.ZeroPad2d((1, 0, 1, 0)) I am Zero padding\n",
        "        # on top and left to keep the output height and width same with the 4×4 kernel\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.model(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdhehrXXJ4Kf"
      },
      "source": [
        "### Hyperparameters\n",
        "\n",
        "The main hyperparameters for the discriminator are, namely, number of output filters, kernel size and stride. A trivial configuration is shown in Table 1. Further tuning is needed when training the model.\n",
        "\n",
        "![Imgur](https://imgur.com/Ia7lyc2.png)\n",
        "\n",
        "We also use padding to maintain the information of pixels on the boundary of the image.\n",
        "\n",
        "-------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CYa-Wl3J4Kg"
      },
      "source": [
        "## So where is PatchGAN Implemented above.\n",
        "\n",
        "Ans is PatchGAN is in-built in the very structure in above.\n",
        "\n",
        "First, noting again that **The main difference between a PatchGAN and a regular GAN discriminator is that - the regular GAN maps an input image to a single scalar output in the range of [0,1], indicating the probability of the image being real or fake, while PatchGAN provides Matrix as the output with each entry signifying whether its corresponding patch is real or fake.**\n",
        "\n",
        "The architecture for our Discriminator here, is as follows:\n",
        "\n",
        "### C64 => C128 => C256 => C512\n",
        "\n",
        "This is referred to as a 3-layer PatchGAN in the CycleGAN and Pix2Pix nomenclature, as excluding the first hidden layer, the model has three hidden layers that could be scaled up or down to give different sized PatchGAN models.\n",
        "\n",
        "In PatchGAN, given for example image of size 256x256, the PatchGAN maps from that 256x256 to an NxN Matrix of outputs X, where each `X_ij` of that NxN Matrix signifies whether the patch `ij` (in X) in the image is real or fake. So each of this `X_ij` value (which is a single scaler value) is a probability for the likelihood that a patch in the input image is real.\n",
        "\n",
        "We can test it with below code, that just calculates the output shape of the Matrix given to a PatchGAN network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "4WmGvq8BJ4Kg",
        "outputId": "55e9e33f-0df1-4a29-87c4-f790b0c1b2b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fourth_layer  7\n",
            "third_layer_input_size  16\n",
            "second_layer_input_size  34\n",
            "first_layer_input_size  70\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Receptive field: 70'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_input_size(output_size, filter_size, stride):\n",
        "    return (output_size - 1) * stride + filter_size\n",
        "\n",
        "# Now invoke above method to calculate the size of various layers in \n",
        "# Discriminator Network\n",
        "\n",
        "last_layer = get_input_size(output_size=1, filter_size=4, stride=1)\n",
        "\n",
        "fourth_layer_input_size = get_input_size(output_size=last_layer, filter_size=4, stride=1)\n",
        "print(\"fourth_layer \", fourth_layer_input_size)\n",
        "\n",
        "\"\"\"Receptive field: 7\"\"\"\n",
        "third_layer_input_size = get_input_size(output_size=fourth_layer_input_size, filter_size=4, stride=2)\n",
        "print(\"third_layer_input_size \", third_layer_input_size)\n",
        "\n",
        "\"\"\"Receptive field: 16\"\"\"\n",
        "second_layer_input_size = get_input_size(output_size=third_layer_input_size, filter_size=4, stride=2)\n",
        "print('second_layer_input_size ', second_layer_input_size)\n",
        "\n",
        "\"\"\"Receptive field: 34\"\"\"\n",
        "first_layer_input_size = get_input_size(output_size=second_layer_input_size, filter_size=4, stride=2)\n",
        "print('first_layer_input_size ', first_layer_input_size)\n",
        "\n",
        "\"\"\"Receptive field: 70\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-03-16T13:53:35.188423Z",
          "iopub.status.busy": "2022-03-16T13:53:35.188143Z",
          "iopub.status.idle": "2022-03-16T13:53:38.725288Z",
          "shell.execute_reply": "2022-03-16T13:53:38.724552Z",
          "shell.execute_reply.started": "2022-03-16T13:53:35.188396Z"
        },
        "id": "YxFuX3PCKOVW",
        "outputId": "7cc9e410-744d-447c-9a5e-79a11bb74253",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CUDA\n"
          ]
        }
      ],
      "source": [
        "# SETUP, LOSS, INITIALIZE MODELS and BUFFERS\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "print(\"Using CUDA\" if cuda else \"Not using CUDA\")\n",
        "\n",
        "# Loss functions\n",
        "# Creating criterion object that will measure the error between the prediction and the target.\n",
        "criterion_GAN = torch.nn.MSELoss()\n",
        "criterion_cycle = torch.nn.L1Loss()\n",
        "criterion_identity = torch.nn.L1Loss()\n",
        "\n",
        "input_shape = (hp.channels, hp.img_size, hp.img_size)\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "Gen_AB = GeneratorResNet(input_shape, hp.num_residual_blocks)\n",
        "Gen_BA = GeneratorResNet(input_shape, hp.num_residual_blocks)\n",
        "Disc_A = Discriminator(input_shape)\n",
        "Disc_B = Discriminator(input_shape)\n",
        "\n",
        "if cuda:\n",
        "    Gen_AB = Gen_AB.cuda()\n",
        "    Gen_BA = Gen_BA.cuda()\n",
        "    Disc_A = Disc_A.cuda()\n",
        "    Disc_B = Disc_B.cuda()\n",
        "    criterion_GAN.cuda()\n",
        "    criterion_cycle.cuda()\n",
        "    criterion_identity.cuda()\n",
        "\n",
        "# Initialize weights\n",
        "Gen_AB.apply(initialize_conv_weights_normal)\n",
        "Gen_BA.apply(initialize_conv_weights_normal)\n",
        "Disc_A.apply(initialize_conv_weights_normal)\n",
        "Disc_B.apply(initialize_conv_weights_normal)\n",
        "\n",
        "# Buffers of previously generated samples\n",
        "fake_A_buffer = ReplayBuffer()\n",
        "fake_B_buffer = ReplayBuffer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T13:53:38.728564Z",
          "iopub.status.busy": "2022-03-16T13:53:38.728362Z",
          "iopub.status.idle": "2022-03-16T13:53:38.738556Z",
          "shell.execute_reply": "2022-03-16T13:53:38.737800Z",
          "shell.execute_reply.started": "2022-03-16T13:53:38.728539Z"
        },
        "id": "TpEIFH7BIRwA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# SAMPLING IMAGES\n",
        "def save_img_samples(batches_done):\n",
        "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
        "    print('batches_done ', batches_done)\n",
        "    imgs = next(iter(val_dataloader))\n",
        "    Gen_AB.eval()\n",
        "    Gen_BA.eval()\n",
        "    real_A = Variable(imgs[\"A\"].type(Tensor))\n",
        "    fake_B = Gen_AB(real_A)\n",
        "    real_B = Variable(imgs[\"B\"].type(Tensor))\n",
        "    fake_A = Gen_BA(real_B)\n",
        "    # Arange images along x-axis\n",
        "    real_A = make_grid(real_A, nrow=16, normalize=True)\n",
        "    real_B = make_grid(real_B, nrow=16, normalize=True)\n",
        "    fake_A = make_grid(fake_A, nrow=16, normalize=True)\n",
        "    fake_B = make_grid(fake_B, nrow=16, normalize=True)\n",
        "    # Arange images along y-axis\n",
        "    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1)    \n",
        "    path =  root_path + \"/%s.png\" % (batches_done)    # Path when running in Google Colab\n",
        "    # path =  '/kaggle/working' + \"/%s.png\" % (batches_done)    # Path when running inside Kaggle\n",
        "    save_image(image_grid, path, normalize=False)    \n",
        "    return path  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svfAVDAXJ4Ki"
      },
      "source": [
        "## Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T13:53:38.741555Z",
          "iopub.status.busy": "2022-03-16T13:53:38.741375Z",
          "iopub.status.idle": "2022-03-16T13:53:38.752145Z",
          "shell.execute_reply": "2022-03-16T13:53:38.751478Z",
          "shell.execute_reply.started": "2022-03-16T13:53:38.741534Z"
        },
        "id": "fgRoX2DbHSwk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "optimizer_G = torch.optim.Adam(\n",
        "    itertools.chain(Gen_AB.parameters(), Gen_BA.parameters()), lr=hp.lr, betas=(hp.b1, hp.b2)\n",
        ")\n",
        "optimizer_Disc_A = torch.optim.Adam(Disc_A.parameters(), lr=hp.lr, betas=(hp.b1, hp.b2))\n",
        "\n",
        "optimizer_Disc_B = torch.optim.Adam(Disc_B.parameters(), lr=hp.lr, betas=(hp.b1, hp.b2))\n",
        "\n",
        "# Learning rate update schedulers\n",
        "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
        "    optimizer_G, lr_lambda=LambdaLR(hp.n_epochs, hp.epoch, hp.decay_start_epoch).step\n",
        ")\n",
        "\n",
        "lr_scheduler_Disc_A = torch.optim.lr_scheduler.LambdaLR(\n",
        "    optimizer_Disc_A, lr_lambda=LambdaLR(hp.n_epochs, hp.epoch, hp.decay_start_epoch).step\n",
        ")\n",
        "\n",
        "lr_scheduler_Disc_B = torch.optim.lr_scheduler.LambdaLR(\n",
        "    optimizer_Disc_B, lr_lambda=LambdaLR(hp.n_epochs, hp.epoch, hp.decay_start_epoch).step\n",
        ")\n",
        "\n",
        "''' So generally both torch.Tensor and torch.cuda.Tensor are equivalent. You can do everything you like with them both.\n",
        "\n",
        "The key difference is just that torch.Tensor occupies CPU memory while torch.cuda.Tensor occupies GPU memory. Of course operations on a CPU Tensor are computed with CPU while operations for the GPU / CUDA Tensor are computed on GPU. '''\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kqrUEbgJNY7o"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    Gen_BA,\n",
        "    Gen_AB,\n",
        "    Disc_A,\n",
        "    Disc_B,\n",
        "    train_dataloader,\n",
        "    n_epochs,\n",
        "    criterion_identity,\n",
        "    criterion_cycle,\n",
        "    lambda_cyc,\n",
        "    criterion_GAN,\n",
        "    optimizer_G,\n",
        "    fake_A_buffer,\n",
        "    fake_B_buffer,\n",
        "    clear_output,\n",
        "    optimizer_Disc_A,\n",
        "    optimizer_Disc_B,\n",
        "    Tensor,\n",
        "    sample_interval,\n",
        "    lambda_id,\n",
        "):\n",
        "    # TRAINING\n",
        "    prev_time = time.time()\n",
        "    for epoch in range(hp.epoch, n_epochs):\n",
        "        for i, batch in enumerate(train_dataloader):\n",
        "\n",
        "            # Set model input\n",
        "            real_A = Variable(batch[\"A\"].type(Tensor))\n",
        "            real_B = Variable(batch[\"B\"].type(Tensor))\n",
        "\n",
        "            # Adversarial ground truths\n",
        "            valid = Variable(\n",
        "                Tensor(np.ones((real_A.size(0), *Disc_A.output_shape))),\n",
        "                requires_grad=False,\n",
        "            )\n",
        "            fake = Variable(\n",
        "                Tensor(np.zeros((real_A.size(0), *Disc_A.output_shape))),\n",
        "                requires_grad=False,\n",
        "            )\n",
        "\n",
        "            #########################\n",
        "            #  Train Generators\n",
        "            #########################\n",
        "\n",
        "            Gen_AB.train() # Gen_AB(real_A) will take real_A and produce fake_B\n",
        "            Gen_BA.train() # Gen_BA(real_B) will take real_B and produce fake_A\n",
        "\n",
        "            \"\"\"\n",
        "            PyTorch stores gradients in a mutable data structure. So we need to set it to a clean state before we use it.\n",
        "            Otherwise, it will have old information from a previous iteration.\n",
        "            \"\"\"\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            # Identity loss\n",
        "            # First pass real_A images to the Genearator, that will generate A-domains images\n",
        "            loss_id_A = criterion_identity(Gen_BA(real_A), real_A)\n",
        "            ''' So Gen_BA() was actually supposed to take real_B and and produce images for domain A\n",
        "            but for calculating Identity loss I will pass to Gen_BA() the target domain images itself\n",
        "            so ideally in this case, Gen_BA() should work as an identity function\n",
        "            i.e. its output should be the input itself. And if there's a difference between \n",
        "            input and output, then that should be my Identity Loss.\n",
        "            \n",
        "            '''\n",
        "            \n",
        "\n",
        "            # Then pass real_B images to the Genearator, that will generate B-domains images\n",
        "            loss_id_B = criterion_identity(Gen_AB(real_B), real_B)\n",
        "\n",
        "            loss_identity = (loss_id_A + loss_id_B) / 2\n",
        "\n",
        "            # GAN losses for GAN_AB\n",
        "            fake_B = Gen_AB(real_A)\n",
        "            loss_GAN_AB = criterion_GAN(Disc_B(fake_B), valid)\n",
        "\n",
        "            # GAN losses for GAN_BA\n",
        "            fake_A = Gen_BA(real_B)\n",
        "            loss_GAN_BA = criterion_GAN(Disc_A(fake_A), valid)\n",
        "\n",
        "            loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
        "\n",
        "            # Cycle Consistency losses\n",
        "            reconstructed_A = Gen_BA(fake_B)\n",
        "\n",
        "            \"\"\"\n",
        "            Forward Cycle Consistency Loss\n",
        "            Forward cycle loss:  lambda * ||G_BtoA(G_AtoB(A)) - A|| (Equation 2 in the paper)\n",
        "            Compute the cycle consistency loss by comparing the reconstructed reconstructed_A images with real real_A  images of domain A.\n",
        "            Lambda for cycle loss is 10.0. Penalizing 10 times and forcing to learn the translation.\n",
        "            \"\"\"\n",
        "            loss_cycle_A = criterion_cycle(reconstructed_A, real_A)\n",
        "\n",
        "            reconstructed_B = Gen_AB(fake_A)\n",
        "\n",
        "            \"\"\"\n",
        "            Backward Cycle Consistency Loss\n",
        "            Backward cycle loss: lambda * ||G_AtoB(G_BtoA(B)) - B|| (Equation 2 of the Paper)\n",
        "            Compute the cycle consistency loss by comparing the reconstructed reconstructed_B images with real real_B images of domain B.\n",
        "            Lambda for cycle loss is 10.0. Penalizing 10 times and forcing to learn the translation.\n",
        "            \"\"\"\n",
        "            loss_cycle_B = criterion_cycle(reconstructed_B, real_B)\n",
        "\n",
        "            loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
        "\n",
        "            \"\"\"\n",
        "            Finally, Total Generators Loss and Back propagation\n",
        "            Add up all the Generators loss and cyclic loss (Equation 3 of paper. Also Equation I the code representation of the equation) and perform backpropagation with optimization.\n",
        "            \"\"\"\n",
        "            loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\n",
        "\n",
        "            loss_G.backward()\n",
        "\n",
        "            \"\"\"\n",
        "            Now we just need to update all the parameters!\n",
        "            Θ_{k+1} = Θ_k − η * ∇_Θ ℓ(y_hat, y)\n",
        "            \"\"\"\n",
        "            optimizer_G.step()\n",
        "\n",
        "            #########################\n",
        "            #  Train Discriminator A\n",
        "            #########################\n",
        "\n",
        "            optimizer_Disc_A.zero_grad()\n",
        "\n",
        "            # Real loss\n",
        "            loss_real = criterion_GAN(Disc_A(real_A), valid)\n",
        "            # Fake loss (on batch of previously generated samples)\n",
        "            fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
        "            loss_fake = criterion_GAN(Disc_A(fake_A_.detach()), fake)\n",
        "\n",
        "            \"\"\" Total loss for Disc_A\n",
        "            And I divide by 2 because as per Paper - \"we divide the objective by 2 while\n",
        "            optimizing D, which slows down the rate at which D learns,\n",
        "            relative to the rate of G.\"\n",
        "            \"\"\"\n",
        "            loss_Disc_A = (loss_real + loss_fake) / 2\n",
        "\n",
        "            \"\"\" do backpropagation i.e.\n",
        "            ∇_Θ will get computed by this call below to backward() \"\"\"\n",
        "            loss_Disc_A.backward()\n",
        "\n",
        "            \"\"\"\n",
        "            Now we just need to update all the parameters!\n",
        "            Θ_{k+1} = Θ_k − η * ∇_Θ ℓ(y_hat, y)\n",
        "            \"\"\"\n",
        "            optimizer_Disc_A.step()\n",
        "\n",
        "            #########################\n",
        "            #  Train Discriminator B\n",
        "            #########################\n",
        "\n",
        "            optimizer_Disc_B.zero_grad()\n",
        "\n",
        "            # Real loss\n",
        "            loss_real = criterion_GAN(Disc_B(real_B), valid)\n",
        "            # Fake loss (on batch of previously generated samples)\n",
        "            fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
        "            loss_fake = criterion_GAN(Disc_B(fake_B_.detach()), fake)\n",
        "\n",
        "            \"\"\" Total loss for Disc_B\n",
        "            And I divide by 2 because as per Paper - \"we divide the objective by 2 while\n",
        "            optimizing D, which slows down the rate at which D learns,\n",
        "            relative to the rate of G.\"\n",
        "            \"\"\"\n",
        "            loss_Disc_B = (loss_real + loss_fake) / 2\n",
        "\n",
        "            \"\"\" do backpropagation i.e.\n",
        "            ∇_Θ will get computed by this call below to backward() \"\"\"\n",
        "            loss_Disc_B.backward()\n",
        "\n",
        "            \"\"\"\n",
        "            Now we just need to update all the parameters!\n",
        "            Θ_{k+1} = Θ_k − η * ∇_Θ ℓ(y_hat, y)\n",
        "            \"\"\"\n",
        "            optimizer_Disc_B.step()\n",
        "\n",
        "            loss_D = (loss_Disc_A + loss_Disc_B) / 2\n",
        "\n",
        "            ##################\n",
        "            #  Log Progress\n",
        "            ##################\n",
        "\n",
        "            # Determine approximate time left\n",
        "            batches_done = epoch * len(train_dataloader) + i\n",
        "            batches_left = n_epochs * len(train_dataloader) - batches_done\n",
        "            time_left = datetime.timedelta(\n",
        "                seconds=batches_left * (time.time() - prev_time)\n",
        "            )\n",
        "            prev_time = time.time()\n",
        "\n",
        "            print(\n",
        "                \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, adv: %f, cycle: %f, identity: %f] ETA: %s\"\n",
        "                % (\n",
        "                    epoch,\n",
        "                    n_epochs,\n",
        "                    i,\n",
        "                    len(train_dataloader),\n",
        "                    loss_D.item(),\n",
        "                    loss_G.item(),\n",
        "                    loss_GAN.item(),\n",
        "                    loss_cycle.item(),\n",
        "                    loss_identity.item(),\n",
        "                    time_left,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # If at sample interval save image\n",
        "            if batches_done % sample_interval == 0:\n",
        "                clear_output()\n",
        "                plot_output(save_img_samples(batches_done), 30, 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2022-03-16T13:53:38.755310Z",
          "iopub.status.busy": "2022-03-16T13:53:38.755057Z",
          "iopub.status.idle": "2022-03-16T14:25:48.463210Z",
          "shell.execute_reply": "2022-03-16T14:25:48.461986Z",
          "shell.execute_reply.started": "2022-03-16T13:53:38.755279Z"
        },
        "id": "GiWH4SwoI7jA",
        "outputId": "3711552e-2bce-4e5b-f003-7db025d02e36",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train(\n",
        "    Gen_BA = Gen_BA,\n",
        "    Gen_AB = Gen_AB,\n",
        "    Disc_A = Disc_A,\n",
        "    Disc_B = Disc_B,\n",
        "    train_dataloader = train_dataloader,\n",
        "    n_epochs = hp.n_epochs,\n",
        "    criterion_identity = criterion_identity,\n",
        "    criterion_cycle = criterion_cycle,\n",
        "    lambda_cyc = hp.lambda_cyc,\n",
        "    criterion_GAN = criterion_GAN,\n",
        "    optimizer_G = optimizer_G,\n",
        "    fake_A_buffer = fake_A_buffer,\n",
        "    fake_B_buffer = fake_B_buffer,\n",
        "    clear_output = clear_output,\n",
        "    optimizer_Disc_A = optimizer_Disc_A,\n",
        "    optimizer_Disc_B = optimizer_Disc_B,\n",
        "    Tensor = Tensor,\n",
        "    sample_interval = hp.sample_interval,\n",
        "    lambda_id = hp.lambda_id,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "MW_Final_Full.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
